{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files in C:\\Users\\Abunch\\Downloads\\Contoso\\.\n",
      "Begin table creation for file: currencyexchange.csv...\n",
      "Scaling factor for currencyexchange.csv: 0.001...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\currencyexchange.csv: 10000000...\n",
      "Inferring data types for column: Date in currencyexchange.csv...\n",
      "Data type inffered for 'Date': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: FromCurrency in currencyexchange.csv...\n",
      "Data type inffered for 'FromCurrency': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: ToCurrency in currencyexchange.csv...\n",
      "Data type inffered for 'ToCurrency': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Exchange in currencyexchange.csv...\n",
      "Data type inffered for 'Exchange': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.currencyexchange (Date DATE, FromCurrency VARCHAR(255), ToCurrency VARCHAR(255), Exchange DECIMAL(18,6));...\n",
      "Begin table creation for file: customer.csv...\n",
      "Scaling factor for customer.csv: 0.001...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\customer.csv: 10000000...\n",
      "Inferring data types for column: CustomerKey in customer.csv...\n",
      "Data type inffered for 'CustomerKey': MEDIUMINT \n",
      " \n",
      " \n",
      "Inferring data types for column: GeoAreaKey in customer.csv...\n",
      "Data type inffered for 'GeoAreaKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: StartDT in customer.csv...\n",
      "Data type inffered for 'StartDT': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: EndDT in customer.csv...\n",
      "Data type inffered for 'EndDT': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: Continent in customer.csv...\n",
      "Data type inffered for 'Continent': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Gender in customer.csv...\n",
      "Data type inffered for 'Gender': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Title in customer.csv...\n",
      "Data type inffered for 'Title': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: GivenName in customer.csv...\n",
      "Data type inffered for 'GivenName': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: MiddleInitial in customer.csv...\n",
      "Data type inffered for 'MiddleInitial': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Surname in customer.csv...\n",
      "Data type inffered for 'Surname': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: StreetAddress in customer.csv...\n",
      "Data type inffered for 'StreetAddress': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: City in customer.csv...\n",
      "Data type inffered for 'City': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: State in customer.csv...\n",
      "Data type inffered for 'State': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: StateFull in customer.csv...\n",
      "Data type inffered for 'StateFull': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: ZipCode in customer.csv...\n",
      "Data type inffered for 'ZipCode': MEDIUMINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Country in customer.csv...\n",
      "Data type inffered for 'Country': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: CountryFull in customer.csv...\n",
      "Data type inffered for 'CountryFull': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Birthday in customer.csv...\n",
      "Data type inffered for 'Birthday': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: Age in customer.csv...\n",
      "Data type inffered for 'Age': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Occupation in customer.csv...\n",
      "Data type inffered for 'Occupation': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Company in customer.csv...\n",
      "Data type inffered for 'Company': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Vehicle in customer.csv...\n",
      "Data type inffered for 'Vehicle': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Latitude in customer.csv...\n",
      "Data type inffered for 'Latitude': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: Longitude in customer.csv...\n",
      "Data type inffered for 'Longitude': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.customer (CustomerKey MEDIUMINT, GeoAreaKey SMALLINT, StartDT DATE, EndDT DATE, Continent VARCHAR(255), Gender VARCHAR(255), Title VARCHAR(255), GivenName VARCHAR(255), MiddleInitial VARCHAR(255), Surname VARCHAR(255), StreetAddress VARCHAR(255), City VARCHAR(255), State VARCHAR(255), StateFull VARCHAR(255), ZipCode MEDIUMINT, Country VARCHAR(255), CountryFull VARCHAR(255), Birthday DATE, Age TINYINT, Occupation VARCHAR(255), Company VARCHAR(255), Vehicle VARCHAR(255), Latitude DECIMAL(18,6), Longitude DECIMAL(18,6));...\n",
      "Begin table creation for file: date.csv...\n",
      "Scaling factor for date.csv: 0.001...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\date.csv: 10000000...\n",
      "Inferring data types for column: Date in date.csv...\n",
      "Data type inffered for 'Date': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: DateKey in date.csv...\n",
      "Data type inffered for 'DateKey': INT \n",
      " \n",
      " \n",
      "Inferring data types for column: Year in date.csv...\n",
      "Data type inffered for 'Year': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: YearQuarter in date.csv...\n",
      "Data type inffered for 'YearQuarter': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearQuarterNumber in date.csv...\n",
      "Data type inffered for 'YearQuarterNumber': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Quarter in date.csv...\n",
      "Data type inffered for 'Quarter': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearMonth in date.csv...\n",
      "Data type inffered for 'YearMonth': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearMonthShort in date.csv...\n",
      "Data type inffered for 'YearMonthShort': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearMonthNumber in date.csv...\n",
      "Data type inffered for 'YearMonthNumber': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Month in date.csv...\n",
      "Data type inffered for 'Month': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: MonthShort in date.csv...\n",
      "Data type inffered for 'MonthShort': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: MonthNumber in date.csv...\n",
      "Data type inffered for 'MonthNumber': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: DayofWeek in date.csv...\n",
      "Data type inffered for 'DayofWeek': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: DayofWeekShort in date.csv...\n",
      "Data type inffered for 'DayofWeekShort': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: DayofWeekNumber in date.csv...\n",
      "Data type inffered for 'DayofWeekNumber': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: WorkingDay in date.csv...\n",
      "Data type inffered for 'WorkingDay': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: WorkingDayNumber in date.csv...\n",
      "Data type inffered for 'WorkingDayNumber': SMALLINT \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.date (Date DATE, DateKey INT, Year SMALLINT, YearQuarter VARCHAR(255), YearQuarterNumber SMALLINT, Quarter VARCHAR(255), YearMonth VARCHAR(255), YearMonthShort VARCHAR(255), YearMonthNumber SMALLINT, Month VARCHAR(255), MonthShort VARCHAR(255), MonthNumber TINYINT, DayofWeek VARCHAR(255), DayofWeekShort VARCHAR(255), DayofWeekNumber TINYINT, WorkingDay TINYINT, WorkingDayNumber SMALLINT);...\n",
      "Begin table creation for file: orderrows.csv...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input must be between 0 and 10,000,000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 522\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# def load_to_table(table_name : str) -> None:\u001b[39;00m\n\u001b[0;32m    490\u001b[0m         \n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# extract_archive(CONTOSO_FILENAME) # works\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# create_database(DBSTART) # works\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m \u001b[43mcreate_tables_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXTRACT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 344\u001b[0m, in \u001b[0;36mcreate_tables_from_csv\u001b[1;34m(targetdirectory)\u001b[0m\n\u001b[0;32m    342\u001b[0m total_row_count \u001b[38;5;241m=\u001b[39m count_rows_in_csv(filepath)\n\u001b[0;32m    343\u001b[0m total_row_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(total_row_count, \u001b[38;5;241m10000000\u001b[39m) \n\u001b[1;32m--> 344\u001b[0m scaling_factor \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_row_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m inferred_types \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaling factor for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaling_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 305\u001b[0m, in \u001b[0;36mcompute_scale\u001b[1;34m(x, max_x, min_scale, max_scale)\u001b[0m\n\u001b[0;32m    303\u001b[0m min_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, x)  \n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (min_x \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_x):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be between 0 and 10,000,000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Normalize using log scale to emphasize early values\u001b[39;00m\n\u001b[0;32m    308\u001b[0m log_x \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog10(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# avoid log(0)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Input must be between 0 and 10,000,000"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import requests\n",
    "from mysql.connector.connection import MySQLConnection\n",
    "from typing import Dict,List,Tuple,Any,Optional\n",
    "import shutil\n",
    "import py7zr  # For handling .7z files\n",
    "import zipfile\n",
    "from tqdm import tqdm # progress bar for download\n",
    "import math\n",
    "\n",
    "\n",
    "# Connection Variables\n",
    "USER = 'Sudo'\n",
    "PASSWORD = 'password'\n",
    "DATABASE = 'sys' # Do not change! This is the default database for MySQL\n",
    "\n",
    "\n",
    "#Server Connection Configuration\n",
    "CONN_CONFIG: Dict[str, str]  = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": USER,\n",
    "    \"password\": PASSWORD,\n",
    "    \"database\": DATABASE\n",
    "}\n",
    "\n",
    "CONTOSO_DOWNLOAD_LINK = r\"https://github.com/sql-bi/Contoso-Data-Generator-V2-Data/releases/download/ready-to-use-data/csv-10m.7z\"\n",
    "CONTOSO_FILENAME = \"csv-10m.7z\"# DO NOT CHANGE THIS VALUE!\n",
    "\n",
    "DBSTART = 'Contoso' # Name of the database to create and where all other tables will be created\n",
    "EXTRACT_DIR = f\"{os.path.expanduser('~')}\\\\Downloads\\\\{DBSTART}\\\\\"\n",
    "\n",
    "\n",
    "def download_file(url: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL and saves it to the user's Downloads folder,\n",
    "    with a progress bar displayed in the console.\n",
    "\n",
    "    :param url: The URL of the file to download.\n",
    "    :param filename: The name of the file to save.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    user_downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    file_path = os.path.join(user_downloads_dir, filename)\n",
    "\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)\n",
    "\n",
    "        # Get the total file size from the headers\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in response.iter_content(chunk_size=1024):  # Download in 1KB chunks\n",
    "                    f.write(chunk)\n",
    "                    progress_bar.update(len(chunk))  # Update the progress bar\n",
    "\n",
    "    print(f\"File downloaded to: {file_path}\")\n",
    "\n",
    "\n",
    "def extract_archive(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Searches for a given .7z archive file in the user's Downloads folder,\n",
    "    and extracts its contents to DOWNLOADS_DIR with a progress bar and detailed error handling.\n",
    "\n",
    "    :param filename: The name of the .7z archive file to extract.\n",
    "    :return: The path where files were extracted or None if extraction failed.\n",
    "    \"\"\"\n",
    "    user_downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    file_path = os.path.join(user_downloads_dir, filename)\n",
    "\n",
    "    # Check if the file exists in Downloads folder\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{filename}' not found in {user_downloads_dir}.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the extraction directory exists\n",
    "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "    try:    \n",
    "        if filename.endswith(\".7z\"):\n",
    "            # Extract .7z files with a progress bar\n",
    "            try:\n",
    "                counter = 0 \n",
    "                with py7zr.SevenZipFile(file_path, mode='r', blocksize=1024*1024 ) as archive:\n",
    "                    file_list = archive.getnames()\n",
    "                    \n",
    "                    with tqdm(total=len(file_list), unit=\"Files Extracted\", desc=\"Extracting\") as  files_progress_bar:\n",
    "                \n",
    "                        for file in file_list:\n",
    "                            try:\n",
    "                                files_progress_bar.set_description(f\"Extracting {file}...\")\n",
    "                                files_progress_bar.refresh()\n",
    "                                archive.extract(targets=[os.path.join(user_downloads_dir,CONTOSO_FILENAME),file], path=EXTRACT_DIR, recursive=False)\n",
    "                                archive.reset() \n",
    "                                files_progress_bar.update(1)\n",
    "                                \n",
    "                            except Exception as file_error:\n",
    "                                print(f\"Error extracting file '{file}': {file_error}\")\n",
    "                                continue\n",
    "                            \n",
    "                            counter += 1\n",
    "                            if counter == len(file_list):\n",
    "                                archive.close()\n",
    "                                break\n",
    "                \n",
    "                print(f\"Extracted '{filename}' to '{EXTRACT_DIR}'.\")\n",
    "                return EXTRACT_DIR\n",
    "           \n",
    "            except py7zr.Bad7zFile:\n",
    "                print(f\"Error: '{filename}' is not a valid 7z file.\")\n",
    "                return None\n",
    "           \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting 7z file '{filename}': {e}\")\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: '{filename}' is not a supported archive format (7z).\")\n",
    "            return None\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied while accessing '{filename}' or writing to '{EXTRACT_DIR}'.\")\n",
    "        return None\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found during extraction.\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while extracting '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_database(db_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new MySQL database if it does not exist.\n",
    "\n",
    "    :param db_name: Name of the database to create.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to MySQL Server (without specifying a database)\n",
    "        conn: MySQLConnection = mysql.connector.connect(**CONN_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create database if it doesn't exist\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS `{db_name}`;\")\n",
    "        print(f\"Database `{db_name}` created or already exists.\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "\n",
    "def infer_mysql_dtype(series: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Infers the MySQL data type based on an analysis of a column's values from CSV data.\n",
    "    \"\"\"\n",
    "    \n",
    "    datetime_formats = {\n",
    "        \"DATETIME\": [\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "            \"%d-%m-%Y %H:%M:%S\",\n",
    "            \"%m/%d/%Y %H:%M:%S\"\n",
    "        ],\n",
    "        \"DATE\": [\n",
    "            \"%Y-%m-%d\",\n",
    "            \"%d-%m-%Y\",\n",
    "            \"%m/%d/%Y\",\n",
    "            \"%d %b %Y\",\n",
    "            \"%d %B %Y\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    non_null_values = series.dropna()\n",
    "    # default if column is empty\n",
    "    if non_null_values.empty:\n",
    "        return \"VARCHAR(255)\"  \n",
    "    def check_datetime(value):\n",
    "        for fmt in datetime_formats[\"DATETIME\"]:\n",
    "            try:\n",
    "                pd.to_datetime(value, format=fmt)\n",
    "                return \"DATETIME\"\n",
    "          \n",
    "            except ValueError:\n",
    "                continue\n",
    "     \n",
    "        for fmt in datetime_formats[\"DATE\"]:\n",
    "            try:\n",
    "                pd.to_datetime(value, format=fmt)\n",
    "                return \"DATE\"\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    inferred_types = {\"int\": 0, \"float\": 0, \"datetime\": 0, \"date\": 0, \"str\": 0, \"bit\":0}\n",
    "    max_str_len = 0\n",
    "    is_bit_candidate = None\n",
    "\n",
    "    for x in non_null_values:\n",
    "        val = x # force native python type\n",
    "        \n",
    "        if isinstance(val, bool):\n",
    "            inferred_types[\"bit\"] += 1\n",
    "        \n",
    "        elif isinstance(val, int) and val in (0, 1):\n",
    "            inferred_types[\"bit\"] += 1\n",
    "        \n",
    "        else:\n",
    "            is_bit_candidate = False \n",
    "        \n",
    "        \n",
    "        if isinstance(val, int):\n",
    "            inferred_types[\"int\"] += 1\n",
    "        \n",
    "        elif isinstance(val, float):\n",
    "            inferred_types[\"float\"] += 1\n",
    "        \n",
    "        elif isinstance(val, str):\n",
    "            max_str_len = max(max_str_len, len(val))\n",
    "            dt_type = check_datetime(val)\n",
    "        \n",
    "            if dt_type == \"DATETIME\":\n",
    "                inferred_types[\"datetime\"] += 1\n",
    "        \n",
    "            elif dt_type == \"DATE\":\n",
    "                inferred_types[\"date\"] += 1\n",
    "        \n",
    "            else:\n",
    "                inferred_types[\"str\"] += 1\n",
    "\n",
    "    # Determine dominant type\n",
    "    \n",
    "    if is_bit_candidate and inferred_types[\"bit\"] == len(non_null_values):\n",
    "        return \"BIT\"\n",
    "    \n",
    "    if inferred_types[\"int\"] == len(non_null_values):\n",
    "        min_val = non_null_values.min()\n",
    "        max_val = non_null_values.max()\n",
    "        \n",
    "        if 0 <= min_val and max_val <= 255:\n",
    "            return \"TINYINT\"\n",
    "        \n",
    "        elif -32768 <= min_val and max_val <= 32767:\n",
    "            return \"SMALLINT\"\n",
    "        \n",
    "        elif -8388608 <= min_val and max_val <= 8388607:\n",
    "            return \"MEDIUMINT\"\n",
    "        \n",
    "        elif -2147483648 <= min_val and max_val <= 2147483647:\n",
    "            return \"INT\"\n",
    "        \n",
    "        else:\n",
    "            return \"BIGINT\"\n",
    "\n",
    "    elif inferred_types[\"float\"] + inferred_types[\"int\"] == len(non_null_values):\n",
    "        return \"DECIMAL(18,6)\"\n",
    "\n",
    "    elif inferred_types[\"datetime\"] > 0 and inferred_types[\"datetime\"] + inferred_types[\"str\"] == len(non_null_values):\n",
    "        return \"DATETIME\"\n",
    "    \n",
    "    elif inferred_types[\"date\"] > 0 and inferred_types[\"date\"] + inferred_types[\"str\"] == len(non_null_values):\n",
    "        return \"DATE\"\n",
    "\n",
    "    elif inferred_types[\"str\"] > 0:\n",
    "    \n",
    "        if max_str_len <= 255:\n",
    "            return \"VARCHAR(255)\"\n",
    "    \n",
    "        elif max_str_len <= 65535:\n",
    "            return \"TEXT\"\n",
    "    \n",
    "        else:\n",
    "            return \"LONGTEXT\"\n",
    "\n",
    "    return \"VARCHAR(255)\"\n",
    "\n",
    "\n",
    "def count_rows_in_csv(filepath: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    :param filepath: Path to the CSV file.\n",
    "    :return: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            total_rows = sum(1 for _ in file) - 1  # Subtract 1 for the header row\n",
    "        return total_rows\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        return 0\n",
    " \n",
    "\n",
    "def compute_scale(x: int, max_x: int = 10000000, min_scale: float = 0.001, max_scale: float = 0.15) -> float:\n",
    "    min_x = max(0, x)  \n",
    "    if not (min_x <= x <= max_x):\n",
    "        raise ValueError(\"Input must be between 0 and 10,000,000\")\n",
    "\n",
    "    # Normalize using log scale to emphasize early values\n",
    "    log_x = math.log10(x + 1)  # avoid log(0)\n",
    "    log_max = math.log10(max_x + 1)\n",
    "    scale = max_scale - (log_x / log_max) * (max_scale - min_scale)\n",
    "    \n",
    "    return round(scale, 6)\n",
    "\n",
    "\n",
    "def create_tables_from_csv(targetdirectory: str = EXTRACT_DIR ) -> None:\n",
    "    \"\"\"\n",
    "    Scans CSV files in a given directory, infers table schema, \n",
    "    and creates MySQL tables dynamically based on CSV headers and data types.\n",
    "    then loads the data into the tables.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(targetdirectory, \"*.csv\"))\n",
    "    file_list = [(os.path.basename(file), file) for file in csv_files]\n",
    "    chunk_size = 1000  # Number of rows per chunk\n",
    "    conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(f\"Found {len(file_list)} CSV files in {targetdirectory}.\")\n",
    "\n",
    "    #######################\n",
    "    # Table Creation Loop #\n",
    "    #######################\n",
    "    for filename, filepath in file_list:\n",
    "\n",
    "        print(f\"Begin table creation for file: {filename}...\")\n",
    "        \n",
    "        table_name = os.path.splitext(filename)[0]  # Remove .csv extension\n",
    "\n",
    "        df = pd.read_csv(filepath,nrows=1)\n",
    "        columns = df.columns.tolist()\n",
    "        total_row_count = count_rows_in_csv(filepath)\n",
    "        total_row_count = max(total_row_count, 9999999) \n",
    "        scaling_factor = compute_scale(x=total_row_count)\n",
    "        inferred_types = []\n",
    "        \n",
    "        print(f\"Scaling factor for {filename}: {scaling_factor}...\")\n",
    "        print(f\"Total rows in {filepath}: {total_row_count}...\")\n",
    "        \n",
    "        for col in columns:\n",
    "            print(f\"Inferring data types for column: {col} in {filename}...\")\n",
    "            column_types = [] \n",
    "\n",
    "            with pd.read_csv(filepath, usecols=[col], chunksize=1000) as data:\n",
    "                \n",
    "                for chunk in data:\n",
    "                    chunk = chunk.sample(frac=scaling_factor)\n",
    "                    column_types.extend([infer_mysql_dtype(chunk[col])])\n",
    "            \n",
    "            most_frequent_type = max(set(column_types), key=column_types.count)\n",
    "            inferred_types.append(most_frequent_type)\n",
    "            print(f\"Data type inffered for '{col}': {most_frequent_type} \\n \\n \")\n",
    "            \n",
    "        \n",
    "        columns_sql = \", \".join(f\"{col} {dtype}\" for col, dtype in zip(columns, inferred_types))\n",
    "        create_table_sql = f\" CREATE TABLE IF NOT EXISTS {DBSTART}.{table_name} ({columns_sql});\"\n",
    "        \n",
    "        print(f\"Executing {create_table_sql}...\")\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "        #Confirm the table was created in the SQL server\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT EXISTS(\n",
    "                SELECT * FROM information_schema.tables \n",
    "                WHERE\n",
    "                    table_type = 'BASE TABLE'\n",
    "                    AND table_name = '{table_name}');\n",
    "            \"\"\"\n",
    "        )\n",
    "        table_check = cursor.fetchall()        \n",
    "        \n",
    "        try:\n",
    "            if len(table_check) == 1 & table_check[0][0] == table_name:\n",
    "                print(f\"Table `{table_name}` created successfully. \\n \\n \")\n",
    "        except Exception as err:\n",
    "            print(f\"Table creation unsuccessfull, Error: {err}\")\n",
    "            \n",
    "        \n",
    "    #####################\n",
    "    # Table Insert Loop #\n",
    "    #####################\n",
    "    for filename, filepath in file_list:            \n",
    "    \n",
    "        table_name = os.path.splitext(filename)[0] \n",
    "        \n",
    "        print(f\"Processing file: {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath,'r') as file:\n",
    "                total_rows = sum(1 for _ in file) - 1 # Subtract 1 for the header row\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{filepath}' not found.\")\n",
    "        \n",
    "        print(f\"Total rows in {filepath}: {total_rows}...\")\n",
    "        print(f\"Chunking....\")     \n",
    "        \n",
    "        total_chunks = math.ceil(total_rows / chunk_size)  # Number of chunks \n",
    "        \n",
    "        print(f\"Total chunks: {total_chunks}...\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Begin loading data into table `{DBSTART}.{table_name}`...\")\n",
    "            #load the CSV to dataframe and load in chunks\n",
    "            with pd.read_csv(filepath, chunksize=chunk_size) as data:\n",
    "                    \n",
    "                with tqdm(total=total_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "                    try:\n",
    "                        for chunk in data:\n",
    "                            columns = chunk.columns.tolist()\n",
    "                            records = [tuple(row) for row in chunk.to_numpy()]\n",
    "                            insert_stmt = f\"\"\"\n",
    "                                INSERT INTO {DBSTART}.{table_name} ({ \", \".join(f\"{col}\" for col in columns)}) \n",
    "                                VALUES ({','.join(['%s'] * len(chunk.columns))});\n",
    "                                \"\"\"\n",
    "                            # print(insert_stmt)\n",
    "                            cursor.executemany(insert_stmt, records)\n",
    "                            conn.commit()\n",
    "                            progress_bar.update(1)\n",
    "                            \n",
    "                    except mysql.connector.Error as err:\n",
    "                        print(f\"Error inserting chunk into table `{table_name}`: {err}\")\n",
    "\n",
    "                    except Exception as err:\n",
    "                        print(f\"Error loading chunk : {err}\")\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading data into table `{table_name}`: {err}\")     \n",
    "               \n",
    "    # Close DB connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_table(databse_name: str, table_name: str, columns: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new MySQL table with the specified columns.\n",
    "\n",
    "    :param table_name: The name of the table to create.\n",
    "    :param columns: A dictionary where keys are column names and values are MySQL data types.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        columns_sql = \", \".join(f\"`{col}` {dtype}\" for col, dtype in columns.items())\n",
    "\n",
    "        create_table_sql = f\"CREATE TABLE IF NOT EXISTS `{databse_name}.{table_name}` ({columns_sql});\"\n",
    "\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "         #Confirm the table was created in the SQL server\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT EXISTS(\n",
    "                SELECT * FROM information_schema.tables \n",
    "                WHERE\n",
    "                        table_type = 'BASE TABLE'\n",
    "                        AND table_name = '{table_name}');\n",
    "            \"\"\"\n",
    "        )\n",
    "        table_check = cursor.fetchall()        \n",
    "        try:\n",
    "            if len(table_check) == 1 & table_check[0][0] == table_name:\n",
    "                print(f\"Table `{table_name}` created successfully.\")\n",
    "        except Exception as err:\n",
    "            print(f\"Table not created successfully, Error: {err}\")\n",
    "            \n",
    "\n",
    "        # Close DB connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "\n",
    "    \n",
    "# def load_to_table(table_name : str) -> None:\n",
    "        \n",
    "#     try:\n",
    "#         #load the CSV to dataframe and index in blocks of 100 rows\n",
    "#         data_df = pd.read_csv(filepath, chunksize=1000)\n",
    "#         # find the number of chunks\n",
    "#         num_chunks = sum(1 for _ in data_df)\n",
    "#         counter = 0\n",
    "        \n",
    "#         with tqdm(total=num_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "\n",
    "#             for chunk in data_df:\n",
    "#                 try:\n",
    "#                     chunk.to_sql(f\"{DBSTART}.{table_name}\", conn, if_exists='append', index=False)\n",
    "#                     print(f\"Chunk of data loaded into table `{table_name}` successfully.\")\n",
    "#                     counter += 1\n",
    "                \n",
    "#                 except Exception as err:\n",
    "#                     print(f\"Error loading chunk # {counter} into table `{table_name}`: {err}\")\n",
    "#                     break\n",
    "                    \n",
    "        \n",
    "#             # Load data into the table\n",
    "#             df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "#             print(f\"Data loaded into table `{table_name}` successfully.\")\n",
    "        \n",
    "    \n",
    "#     except Exception as err:\n",
    "#         print(f\"Error loading data into table `{table_name}`: {err}\")\n",
    "\n",
    "# download_file(CONTOSO_DOWNLOAD_LINK, CONTOSO_FILENAME) # works\n",
    "# extract_archive(CONTOSO_FILENAME) # works\n",
    "# create_database(DBSTART) # works\n",
    "create_tables_from_csv(EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: date.csv...\n",
      "Total rows in C:\\\\Users\\\\Abunch\\\\Downloads\\\\Contoso\\\\date.csv: 3653...\n",
      "Chunking....\n",
      "Total chunks: 37...\n",
      "Begin loading data into table `Contoso.date`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading date: 100%|██████████| 37/37 [00:00<00:00, 66.92chunks/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "    \n",
    "csv_files = [r\"C:\\\\Users\\\\Abunch\\\\Downloads\\\\Contoso\\\\date.csv\"]        \n",
    "file_list = [(os.path.basename(file), file) for file in csv_files]\n",
    "conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "# Table Insert Loop\n",
    "for filename, filepath in file_list:            \n",
    "    \n",
    "    table_name = os.path.splitext(filename)[0] \n",
    "    \n",
    "    print(f\"Processing file: {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath,'r') as file:\n",
    "            total_rows = sum(1 for _ in file) - 1 # Subtract 1 for the header row\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        total_rows = 0\n",
    "    \n",
    "    print(f\"Total rows in {filepath}: {total_rows}...\")\n",
    "    print(f\"Chunking....\")     \n",
    "    \n",
    "    chunk_size = 100  # Number of rows per chunk\n",
    "    total_chunks = math.ceil(total_rows / chunk_size)  # Number of chunks \n",
    "    counter = 1\n",
    "    \n",
    "    print(f\"Total chunks: {total_chunks}...\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Begin loading data into table `{DBSTART}.{table_name}`...\")\n",
    "        #load the CSV to dataframe and load in chunks\n",
    "        with pd.read_csv(filepath, chunksize=chunk_size) as data:\n",
    "                \n",
    "            with tqdm(total=total_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "                try:\n",
    "                    for chunk in data:\n",
    "                        columns = chunk.columns.tolist()\n",
    "                        records = [tuple(row) for row in chunk.to_numpy()]\n",
    "                        insert_stmt = f\"\"\"\n",
    "                            INSERT INTO {DBSTART}.{table_name} ({ \", \".join(f\"{col}\" for col in columns)}) \n",
    "                            VALUES ({','.join(['%s'] * len(chunk.columns))})\n",
    "                            \"\"\"\n",
    "                        # print(insert_stmt)\n",
    "                        cursor.executemany(insert_stmt, records)\n",
    "                        conn.commit()\n",
    "                        progress_bar.update(1)\n",
    "                        counter += 1\n",
    "                        \n",
    "                except mysql.connector.Error as err:\n",
    "                    print(f\"Error inserting chunk into table `{table_name}`: {err}\")\n",
    "                    break\n",
    "\n",
    "                except Exception as err:\n",
    "                    print(f\"Error loading chunk : {err}\")\n",
    "                    break\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error loading data into table `{table_name}`: {err}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
