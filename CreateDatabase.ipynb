{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files in C:\\Users\\Abunch\\Downloads\\Contoso\\.\n",
      "Processing file: currencyexchange.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\currencyexchange.csv: 91325...\n",
      "Chunking....\n",
      "Total chunks: 92...\n",
      "Begin loading data into table `Contoso.currencyexchange`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading currencyexchange: 100%|██████████| 92/92 [00:03<00:00, 25.61chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: customer.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\customer.csv: 1679846...\n",
      "Chunking....\n",
      "Total chunks: 1680...\n",
      "Begin loading data into table `Contoso.customer`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading customer: 100%|██████████| 1680/1680 [03:24<00:00,  8.22chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: date.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\date.csv: 3653...\n",
      "Chunking....\n",
      "Total chunks: 4...\n",
      "Begin loading data into table `Contoso.date`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading date: 100%|██████████| 4/4 [00:00<00:00, 12.68chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: orderrows.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\orderrows.csv: 13042913...\n",
      "Chunking....\n",
      "Total chunks: 13043...\n",
      "Begin loading data into table `Contoso.orderrows`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading orderrows: 100%|██████████| 13043/13043 [09:34<00:00, 22.70chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: orders.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\orders.csv: 8833576...\n",
      "Chunking....\n",
      "Total chunks: 8834...\n",
      "Begin loading data into table `Contoso.orders`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading orders: 100%|██████████| 8834/8834 [06:35<00:00, 22.33chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: product.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\product.csv: 2517...\n",
      "Chunking....\n",
      "Total chunks: 3...\n",
      "Begin loading data into table `Contoso.product`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading product: 100%|██████████| 3/3 [00:00<00:00,  7.88chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: sales.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\sales.csv: 21170416...\n",
      "Chunking....\n",
      "Total chunks: 21171...\n",
      "Begin loading data into table `Contoso.sales`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sales: 100%|██████████| 21171/21171 [19:21<00:00, 18.23chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: store.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\store.csv: 74...\n",
      "Chunking....\n",
      "Total chunks: 1...\n",
      "Begin loading data into table `Contoso.store`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading store: 100%|██████████| 1/1 [00:00<00:00, 68.08chunks/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import requests\n",
    "from mysql.connector.connection import MySQLConnection\n",
    "from typing import Dict,List,Tuple,Any,Optional\n",
    "import shutil\n",
    "import py7zr  # For handling .7z files\n",
    "import zipfile\n",
    "from tqdm import tqdm # progress bar for download\n",
    "import math\n",
    "\n",
    "\n",
    "# Connection Variables\n",
    "USER = 'Sudo'\n",
    "PASSWORD = 'password'\n",
    "DATABASE = 'sys' # Do not change! This is the default database for MySQL\n",
    "\n",
    "\n",
    "#Server Connection Configuration\n",
    "CONN_CONFIG: Dict[str, str]  = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": USER,\n",
    "    \"password\": PASSWORD,\n",
    "    \"database\": DATABASE\n",
    "}\n",
    "\n",
    "CONTOSO_DOWNLOAD_LINK = r\"https://github.com/sql-bi/Contoso-Data-Generator-V2-Data/releases/download/ready-to-use-data/csv-10m.7z\"\n",
    "CONTOSO_FILENAME = \"csv-10m.7z\"# DO NOT CHANGE THIS VALUE!\n",
    "\n",
    "DBSTART = 'Contoso' # Name of the database to create and where all other tables will be created\n",
    "EXTRACT_DIR = f\"{os.path.expanduser('~')}\\\\Downloads\\\\{DBSTART}\\\\\"\n",
    "\n",
    "\n",
    "def download_file(url: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL and saves it to the user's Downloads folder,\n",
    "    with a progress bar displayed in the console.\n",
    "\n",
    "    :param url: The URL of the file to download.\n",
    "    :param filename: The name of the file to save.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    user_downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    file_path = os.path.join(user_downloads_dir, filename)\n",
    "\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)\n",
    "\n",
    "        # Get the total file size from the headers\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in response.iter_content(chunk_size=1024):  # Download in 1KB chunks\n",
    "                    f.write(chunk)\n",
    "                    progress_bar.update(len(chunk))  # Update the progress bar\n",
    "\n",
    "    print(f\"File downloaded to: {file_path}\")\n",
    "\n",
    "\n",
    "def extract_archive(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Searches for a given .7z archive file in the user's Downloads folder,\n",
    "    and extracts its contents to DOWNLOADS_DIR with a progress bar and detailed error handling.\n",
    "\n",
    "    :param filename: The name of the .7z archive file to extract.\n",
    "    :return: The path where files were extracted or None if extraction failed.\n",
    "    \"\"\"\n",
    "    user_downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    file_path = os.path.join(user_downloads_dir, filename)\n",
    "\n",
    "    # Check if the file exists in Downloads folder\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{filename}' not found in {user_downloads_dir}.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the extraction directory exists\n",
    "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "    try:    \n",
    "        if filename.endswith(\".7z\"):\n",
    "            # Extract .7z files with a progress bar\n",
    "            try:\n",
    "                counter = 0 \n",
    "                with py7zr.SevenZipFile(file_path, mode='r', blocksize=1024*1024 ) as archive:\n",
    "                    file_list = archive.getnames()\n",
    "                    \n",
    "                    with tqdm(total=len(file_list), unit=\"Files Extracted\", desc=\"Extracting\") as  files_progress_bar:\n",
    "                \n",
    "                        for file in file_list:\n",
    "                            try:\n",
    "                                files_progress_bar.set_description(f\"Extracting {file}...\")\n",
    "                                files_progress_bar.refresh()\n",
    "                                archive.extract(targets=[os.path.join(user_downloads_dir,CONTOSO_FILENAME),file], path=EXTRACT_DIR, recursive=False)\n",
    "                                archive.reset() \n",
    "                                files_progress_bar.update(1)\n",
    "                                \n",
    "                            except Exception as file_error:\n",
    "                                print(f\"Error extracting file '{file}': {file_error}\")\n",
    "                                continue\n",
    "                            \n",
    "                            counter += 1\n",
    "                            if counter == len(file_list):\n",
    "                                archive.close()\n",
    "                                break\n",
    "                \n",
    "                print(f\"Extracted '{filename}' to '{EXTRACT_DIR}'.\")\n",
    "                return EXTRACT_DIR\n",
    "           \n",
    "            except py7zr.Bad7zFile:\n",
    "                print(f\"Error: '{filename}' is not a valid 7z file.\")\n",
    "                return None\n",
    "           \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting 7z file '{filename}': {e}\")\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: '{filename}' is not a supported archive format (7z).\")\n",
    "            return None\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied while accessing '{filename}' or writing to '{EXTRACT_DIR}'.\")\n",
    "        return None\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found during extraction.\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while extracting '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_database(db_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new MySQL database if it does not exist.\n",
    "\n",
    "    :param db_name: Name of the database to create.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to MySQL Server (without specifying a database)\n",
    "        conn: MySQLConnection = mysql.connector.connect(**CONN_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create database if it doesn't exist\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS `{db_name}`;\")\n",
    "        print(f\"Database `{db_name}` created or already exists.\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "\n",
    "def infer_mysql_dtype(series: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Infers the MySQL data type based on an analysis of a column's values from CSV data.\n",
    "    \"\"\"\n",
    "    \n",
    "    datetime_formats = {\n",
    "        \"DATETIME\": [\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "            \"%d-%m-%Y %H:%M:%S\",\n",
    "            \"%m/%d/%Y %H:%M:%S\"\n",
    "        ],\n",
    "        \"DATE\": [\n",
    "            \"%Y-%m-%d\",\n",
    "            \"%d-%m-%Y\",\n",
    "            \"%m/%d/%Y\",\n",
    "            \"%d %b %Y\",\n",
    "            \"%d %B %Y\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    non_null_values = series.dropna()\n",
    "    # default if column is empty\n",
    "    if non_null_values.empty:\n",
    "        return \"VARCHAR(255)\"\n",
    "      \n",
    "    def check_datetime(value):\n",
    "        for fmt in datetime_formats[\"DATETIME\"]:\n",
    "            try:\n",
    "                pd.to_datetime(value, format=fmt)\n",
    "                return \"DATETIME\"\n",
    "          \n",
    "            except ValueError:\n",
    "                continue\n",
    "     \n",
    "        for fmt in datetime_formats[\"DATE\"]:\n",
    "            try:\n",
    "                pd.to_datetime(value, format=fmt)\n",
    "                return \"DATE\"\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    inferred_types = {\"int\": 0, \"float\": 0, \"datetime\": 0, \"date\": 0, \"str\": 0, \"bit\":0}\n",
    "    max_str_len = 0\n",
    "    is_bit_candidate = None\n",
    "\n",
    "    for x in non_null_values:\n",
    "        val = x # force native python type\n",
    "        \n",
    "        if isinstance(val, bool):\n",
    "            inferred_types[\"bit\"] += 1\n",
    "        \n",
    "        elif isinstance(val, int) and val in (0, 1):\n",
    "            inferred_types[\"bit\"] += 1\n",
    "        \n",
    "        else:\n",
    "            is_bit_candidate = False \n",
    "        \n",
    "        \n",
    "        if isinstance(val, int):\n",
    "            inferred_types[\"int\"] += 1\n",
    "        \n",
    "        elif isinstance(val, float):\n",
    "            inferred_types[\"float\"] += 1\n",
    "        \n",
    "        elif isinstance(val, str):\n",
    "            max_str_len = max(max_str_len, len(val))\n",
    "            dt_type = check_datetime(val)\n",
    "        \n",
    "            if dt_type == \"DATETIME\":\n",
    "                inferred_types[\"datetime\"] += 1\n",
    "        \n",
    "            elif dt_type == \"DATE\":\n",
    "                inferred_types[\"date\"] += 1\n",
    "        \n",
    "            else:\n",
    "                inferred_types[\"str\"] += 1\n",
    "\n",
    "    # Determine dominant type\n",
    "    \n",
    "    if is_bit_candidate and inferred_types[\"bit\"] == len(non_null_values):\n",
    "        return \"BIT\"\n",
    "    \n",
    "    if inferred_types[\"int\"] == len(non_null_values):\n",
    "            return \"INT\"\n",
    "\n",
    "    elif inferred_types[\"float\"] + inferred_types[\"int\"] == len(non_null_values):\n",
    "        return \"DECIMAL(18,6)\"\n",
    "\n",
    "    elif inferred_types[\"datetime\"] > 0 and inferred_types[\"datetime\"] + inferred_types[\"str\"] == len(non_null_values):\n",
    "        return \"DATETIME\"\n",
    "    \n",
    "    elif inferred_types[\"date\"] > 0 and inferred_types[\"date\"] + inferred_types[\"str\"] == len(non_null_values):\n",
    "        return \"DATE\"\n",
    "\n",
    "    elif inferred_types[\"str\"] > 0:\n",
    "    \n",
    "        if max_str_len <= 255:\n",
    "            return f\"VARCHAR(255)\"\n",
    "    \n",
    "        elif max_str_len <= 65535:\n",
    "            return \"TEXT\"\n",
    "    \n",
    "        else:\n",
    "            return \"LONGTEXT\"\n",
    "\n",
    "    return \"VARCHAR(255)\"\n",
    "\n",
    "\n",
    "def count_rows_in_csv(filepath: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    :param filepath: Path to the CSV file.\n",
    "    :return: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            total_rows = sum(1 for _ in file) - 1  # Subtract 1 for the header row\n",
    "        return total_rows\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        return 0\n",
    " \n",
    "\n",
    "def compute_scale(x: int, min_x: int = 0, max_x: int = 10000000, min_scale: float = 0.01, max_scale: float = 0.20) -> float:\n",
    "    if not (min_x <= x <= max_x):\n",
    "        raise ValueError(\"Input must be between 0 and 10,000,000\")\n",
    "\n",
    "    # Normalize using log scale to emphasize early values\n",
    "    log_x = math.log10(x + 1)  # avoid log(0)\n",
    "    log_max = math.log10(max_x + 1)\n",
    "    scale = max_scale - (log_x / log_max) * (max_scale - min_scale)\n",
    "    \n",
    "    return round(scale, 6)\n",
    "\n",
    "\n",
    "def create_tables_from_csv(targetdirectory: str = EXTRACT_DIR ) -> None:\n",
    "    \"\"\"\n",
    "    Scans CSV files in a given directory, infers table schema, \n",
    "    and creates MySQL tables dynamically based on CSV headers and data types.\n",
    "    then loads the data into the tables.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(targetdirectory, \"*.csv\"))\n",
    "    file_list = [(os.path.basename(file), file) for file in csv_files]\n",
    "    chunk_size = 1000  # Number of rows per chunk\n",
    "    conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(f\"Found {len(file_list)} CSV files in {targetdirectory}.\")\n",
    "\n",
    "    # #######################\n",
    "    # # Table Creation Loop #\n",
    "    # #######################\n",
    "    # for filename, filepath in file_list:\n",
    "\n",
    "    #     print(f\"Begin table creation for file: {filename}...\")\n",
    "        \n",
    "    #     table_name = os.path.splitext(filename)[0]  # Remove .csv extension\n",
    "\n",
    "    #     df = pd.read_csv(filepath,nrows=1)\n",
    "    #     columns = df.columns.tolist()\n",
    "    #     total_row_count = count_rows_in_csv(filepath)\n",
    "    #     total_row_count = min(total_row_count, 10000000)  # Limit to compute scale max_x default.\n",
    "    #     scaling_factor = compute_scale(x=total_row_count)\n",
    "    #     inferred_types = []\n",
    "        \n",
    "    #     print(f\"Scaling factor for {filename}: {scaling_factor}...\")\n",
    "    #     print(f\"Total rows in {filepath}: {total_row_count}...\")\n",
    "        \n",
    "    #     for col in columns:\n",
    "    #         print(f\"Inferring data types for column: {col} in {filename}...\")\n",
    "    #         column_types = [] \n",
    "\n",
    "    #         with pd.read_csv(filepath, usecols=[col], chunksize=1000) as data:\n",
    "                \n",
    "    #             for chunk in data:\n",
    "    #                 chunk = chunk.sample(frac=scaling_factor)\n",
    "    #                 column_types.extend([infer_mysql_dtype(chunk[col])])\n",
    "            \n",
    "    #         # prefer VARCHAR over INT\n",
    "    #         if \"VARCHAR(255)\" in column_types:\n",
    "    #             most_frequent_type = \"VARCHAR(255)\"\n",
    "\n",
    "    #         else:\n",
    "    #             most_frequent_type = max(set(column_types), key=column_types.count)\n",
    "            \n",
    "    #         inferred_types.append(most_frequent_type)\n",
    "    #         print(f\"Data type inffered for '{col}': {most_frequent_type} \\n \\n \")\n",
    "            \n",
    "        \n",
    "    #     columns_sql = \", \".join(f\"{col} {dtype}\" for col, dtype in zip(columns, inferred_types))\n",
    "    #     create_table_sql = f\" CREATE TABLE IF NOT EXISTS {DBSTART}.{table_name} ({columns_sql});\"\n",
    "        \n",
    "    #     print(f\"Executing {create_table_sql}...\")\n",
    "    #     cursor.execute(create_table_sql)\n",
    "        \n",
    "    #     #Confirm the table was created in the SQL server\n",
    "    #     cursor.execute(\n",
    "    #         f\"\"\"\n",
    "    #         SELECT EXISTS(\n",
    "    #             SELECT * FROM information_schema.tables \n",
    "    #             WHERE\n",
    "    #                 table_type = 'BASE TABLE'\n",
    "    #                 AND table_name = '{table_name}');\n",
    "    #         \"\"\"\n",
    "    #     )\n",
    "    #     table_check = cursor.fetchall()        \n",
    "        \n",
    "    #     try:\n",
    "    #         if len(table_check) == 1 & table_check[0][0] == table_name:\n",
    "    #             print(f\"Table `{table_name}` created successfully. \\n \\n \")\n",
    "    #     except Exception as err:\n",
    "    #         print(f\"Table creation unsuccessfull, Error: {err}\")\n",
    "            \n",
    "        \n",
    "    #####################\n",
    "    # Table Insert Loop #\n",
    "    #####################\n",
    "    for filename, filepath in file_list:            \n",
    "    \n",
    "        table_name = os.path.splitext(filename)[0] \n",
    "        \n",
    "        print(f\"Processing file: {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath,'r') as file:\n",
    "                total_rows = sum(1 for _ in file) - 1 # Subtract 1 for the header row\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{filepath}' not found.\")\n",
    "        \n",
    "        print(f\"Total rows in {filepath}: {total_rows}...\")\n",
    "        print(f\"Chunking....\")     \n",
    "        \n",
    "        total_chunks = math.ceil(total_rows / chunk_size)  # Number of chunks \n",
    "        \n",
    "        print(f\"Total chunks: {total_chunks}...\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Begin loading data into table `{DBSTART}.{table_name}`...\")\n",
    "            #load the CSV to dataframe and load in chunks\n",
    "            with pd.read_csv(filepath, chunksize=chunk_size) as data:\n",
    "                    \n",
    "                with tqdm(total=total_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "                    try:\n",
    "                        for chunk in data:\n",
    "                            columns = chunk.columns.tolist()\n",
    "                            # helps force native python types for conversion in mysql\n",
    "                            records = [tuple(row.tolist()) for row in chunk.to_numpy()] \n",
    "                            insert_stmt = f\"\"\"\n",
    "                                INSERT INTO {DBSTART}.{table_name} ({ \", \".join(f\"{col}\" for col in columns)}) \n",
    "                                VALUES ({','.join(['%s'] * len(chunk.columns))});\n",
    "                                \"\"\"\n",
    "                            # print(insert_stmt)\n",
    "                            cursor.executemany(insert_stmt, records)\n",
    "                            conn.commit()\n",
    "                            progress_bar.update(1)\n",
    "                            \n",
    "                    except mysql.connector.Error as err:\n",
    "                        print(f\"Error inserting chunk into table `{table_name}`: {err}\")\n",
    "\n",
    "                    except Exception as err:\n",
    "                        print(f\"Error loading chunk : {err}\")\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading data into table `{table_name}`: {err}\")     \n",
    "               \n",
    "    # Close DB connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_table(databse_name: str, table_name: str, columns: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new MySQL table with the specified columns.\n",
    "\n",
    "    :param table_name: The name of the table to create.\n",
    "    :param columns: A dictionary where keys are column names and values are MySQL data types.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        columns_sql = \", \".join(f\"`{col}` {dtype}\" for col, dtype in columns.items())\n",
    "\n",
    "        create_table_sql = f\"CREATE TABLE IF NOT EXISTS `{databse_name}.{table_name}` ({columns_sql});\"\n",
    "\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "         #Confirm the table was created in the SQL server\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT EXISTS(\n",
    "                SELECT * FROM information_schema.tables \n",
    "                WHERE\n",
    "                        table_type = 'BASE TABLE'\n",
    "                        AND table_name = '{table_name}');\n",
    "            \"\"\"\n",
    "        )\n",
    "        table_check = cursor.fetchall()        \n",
    "        try:\n",
    "            if len(table_check) == 1 & table_check[0][0] == table_name:\n",
    "                print(f\"Table `{table_name}` created successfully.\")\n",
    "        except Exception as err:\n",
    "            print(f\"Table not created successfully, Error: {err}\")\n",
    "            \n",
    "\n",
    "        # Close DB connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "\n",
    "    \n",
    "# def load_to_table(table_name : str) -> None:\n",
    "        \n",
    "#     try:\n",
    "#         #load the CSV to dataframe and index in blocks of 100 rows\n",
    "#         data_df = pd.read_csv(filepath, chunksize=1000)\n",
    "#         # find the number of chunks\n",
    "#         num_chunks = sum(1 for _ in data_df)\n",
    "#         counter = 0\n",
    "        \n",
    "#         with tqdm(total=num_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "\n",
    "#             for chunk in data_df:\n",
    "#                 try:\n",
    "#                     chunk.to_sql(f\"{DBSTART}.{table_name}\", conn, if_exists='append', index=False)\n",
    "#                     print(f\"Chunk of data loaded into table `{table_name}` successfully.\")\n",
    "#                     counter += 1\n",
    "                \n",
    "#                 except Exception as err:\n",
    "#                     print(f\"Error loading chunk # {counter} into table `{table_name}`: {err}\")\n",
    "#                     break\n",
    "                    \n",
    "        \n",
    "#             # Load data into the table\n",
    "#             df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "#             print(f\"Data loaded into table `{table_name}` successfully.\")\n",
    "        \n",
    "    \n",
    "#     except Exception as err:\n",
    "#         print(f\"Error loading data into table `{table_name}`: {err}\")\n",
    "\n",
    "# download_file(CONTOSO_DOWNLOAD_LINK, CONTOSO_FILENAME)\n",
    "# extract_archive(CONTOSO_FILENAME)\n",
    "# create_database(DBSTART)\n",
    "create_tables_from_csv(EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: date.csv...\n",
      "Total rows in C:\\\\Users\\\\Abunch\\\\Downloads\\\\Contoso\\\\date.csv: 3653...\n",
      "Chunking....\n",
      "Total chunks: 37...\n",
      "Begin loading data into table `Contoso.date`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading date: 100%|██████████| 37/37 [00:00<00:00, 66.92chunks/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def compute_scale(x: int, min_x: int = 0, max_x: int = 10000000, min_scale: float = 0.01, max_scale: float = 0.20) -> float:\n",
    "    if not (min_x <= x <= max_x):\n",
    "        raise ValueError(\"Input must be between 0 and 10,000,000\")\n",
    "    \n",
    "    log_x = math.log10(x + 1)  # avoid log(0)\n",
    "    log_max = math.log10(max_x + 1)\n",
    "    scale = max_scale - (log_x / log_max) * (max_scale - min_scale)\n",
    "    \n",
    "    return round(scale, 6)\n",
    "\n",
    "row_counts = range(1, 1000001, 100) \n",
    "scaling_factors = [compute_scale(x) for x in row_counts]\n",
    "sampled_volumes = [x * scale for x, scale in zip(row_counts, scaling_factors)] \n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "ax1.plot(row_counts, scaling_factors, label=\"Scaling Factor\", color=\"blue\")\n",
    "ax1.set_xlabel(\"Row Count\")\n",
    "ax1.set_ylabel(\"Scaling Factor\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "ax1.grid(True)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(row_counts, sampled_volumes, label=\"Sampled Volume\", color=\"red\")\n",
    "ax2.set_ylabel(\"Sampled Volume (Scaling Factor * Row Count)\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "plt.title(\"Relationship Between Row Count, Scaling Factor, and Sampled Volume\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
