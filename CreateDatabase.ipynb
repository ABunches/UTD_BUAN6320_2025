{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files in C:\\Users\\Abunch\\Downloads\\Contoso\\.\n",
      "Begin table creation for file: currencyexchange.csv...\n",
      "Scaling factor for currencyexchange.csv: 0.04441...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\currencyexchange.csv: 91325...\n",
      "Inferring data types for column: Date in currencyexchange.csv...\n",
      "Data type inffered for 'Date': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: FromCurrency in currencyexchange.csv...\n",
      "Data type inffered for 'FromCurrency': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: ToCurrency in currencyexchange.csv...\n",
      "Data type inffered for 'ToCurrency': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Exchange in currencyexchange.csv...\n",
      "Data type inffered for 'Exchange': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.currencyexchange (Date DATE, FromCurrency VARCHAR(255), ToCurrency VARCHAR(255), Exchange DECIMAL(18,6));...\n",
      "Begin table creation for file: customer.csv...\n",
      "Scaling factor for customer.csv: 0.017491...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\customer.csv: 1679846...\n",
      "Inferring data types for column: CustomerKey in customer.csv...\n",
      "Data type inffered for 'CustomerKey': MEDIUMINT \n",
      " \n",
      " \n",
      "Inferring data types for column: GeoAreaKey in customer.csv...\n",
      "Data type inffered for 'GeoAreaKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: StartDT in customer.csv...\n",
      "Data type inffered for 'StartDT': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: EndDT in customer.csv...\n",
      "Data type inffered for 'EndDT': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: Continent in customer.csv...\n",
      "Data type inffered for 'Continent': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Gender in customer.csv...\n",
      "Data type inffered for 'Gender': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Title in customer.csv...\n",
      "Data type inffered for 'Title': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: GivenName in customer.csv...\n",
      "Data type inffered for 'GivenName': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: MiddleInitial in customer.csv...\n",
      "Data type inffered for 'MiddleInitial': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Surname in customer.csv...\n",
      "Data type inffered for 'Surname': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: StreetAddress in customer.csv...\n",
      "Data type inffered for 'StreetAddress': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: City in customer.csv...\n",
      "Data type inffered for 'City': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: State in customer.csv...\n",
      "Data type inffered for 'State': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: StateFull in customer.csv...\n",
      "Data type inffered for 'StateFull': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: ZipCode in customer.csv...\n",
      "Data type inffered for 'ZipCode': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Country in customer.csv...\n",
      "Data type inffered for 'Country': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: CountryFull in customer.csv...\n",
      "Data type inffered for 'CountryFull': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Birthday in customer.csv...\n",
      "Data type inffered for 'Birthday': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: Age in customer.csv...\n",
      "Data type inffered for 'Age': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Occupation in customer.csv...\n",
      "Data type inffered for 'Occupation': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Company in customer.csv...\n",
      "Data type inffered for 'Company': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Vehicle in customer.csv...\n",
      "Data type inffered for 'Vehicle': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Latitude in customer.csv...\n",
      "Data type inffered for 'Latitude': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: Longitude in customer.csv...\n",
      "Data type inffered for 'Longitude': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.customer (CustomerKey MEDIUMINT, GeoAreaKey SMALLINT, StartDT DATE, EndDT DATE, Continent VARCHAR(255), Gender VARCHAR(255), Title VARCHAR(255), GivenName VARCHAR(255), MiddleInitial VARCHAR(255), Surname VARCHAR(255), StreetAddress VARCHAR(255), City VARCHAR(255), State VARCHAR(255), StateFull VARCHAR(255), ZipCode VARCHAR(255), Country VARCHAR(255), CountryFull VARCHAR(255), Birthday DATE, Age TINYINT, Occupation VARCHAR(255), Company VARCHAR(255), Vehicle VARCHAR(255), Latitude DECIMAL(18,6), Longitude DECIMAL(18,6));...\n",
      "Begin table creation for file: date.csv...\n",
      "Scaling factor for date.csv: 0.074164...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\date.csv: 3653...\n",
      "Inferring data types for column: Date in date.csv...\n",
      "Data type inffered for 'Date': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: DateKey in date.csv...\n",
      "Data type inffered for 'DateKey': INT \n",
      " \n",
      " \n",
      "Inferring data types for column: Year in date.csv...\n",
      "Data type inffered for 'Year': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: YearQuarter in date.csv...\n",
      "Data type inffered for 'YearQuarter': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearQuarterNumber in date.csv...\n",
      "Data type inffered for 'YearQuarterNumber': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Quarter in date.csv...\n",
      "Data type inffered for 'Quarter': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearMonth in date.csv...\n",
      "Data type inffered for 'YearMonth': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearMonthShort in date.csv...\n",
      "Data type inffered for 'YearMonthShort': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: YearMonthNumber in date.csv...\n",
      "Data type inffered for 'YearMonthNumber': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Month in date.csv...\n",
      "Data type inffered for 'Month': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: MonthShort in date.csv...\n",
      "Data type inffered for 'MonthShort': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: MonthNumber in date.csv...\n",
      "Data type inffered for 'MonthNumber': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: DayofWeek in date.csv...\n",
      "Data type inffered for 'DayofWeek': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: DayofWeekShort in date.csv...\n",
      "Data type inffered for 'DayofWeekShort': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: DayofWeekNumber in date.csv...\n",
      "Data type inffered for 'DayofWeekNumber': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: WorkingDay in date.csv...\n",
      "Data type inffered for 'WorkingDay': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: WorkingDayNumber in date.csv...\n",
      "Data type inffered for 'WorkingDayNumber': SMALLINT \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.date (Date DATE, DateKey INT, Year SMALLINT, YearQuarter VARCHAR(255), YearQuarterNumber SMALLINT, Quarter VARCHAR(255), YearMonth VARCHAR(255), YearMonthShort VARCHAR(255), YearMonthNumber SMALLINT, Month VARCHAR(255), MonthShort VARCHAR(255), MonthNumber TINYINT, DayofWeek VARCHAR(255), DayofWeekShort VARCHAR(255), DayofWeekNumber TINYINT, WorkingDay TINYINT, WorkingDayNumber SMALLINT);...\n",
      "Begin table creation for file: orderrows.csv...\n",
      "Scaling factor for orderrows.csv: 0.001...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\orderrows.csv: 10000000...\n",
      "Inferring data types for column: OrderKey in orderrows.csv...\n",
      "Data type inffered for 'OrderKey': INT \n",
      " \n",
      " \n",
      "Inferring data types for column: LineNumber in orderrows.csv...\n",
      "Data type inffered for 'LineNumber': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: ProductKey in orderrows.csv...\n",
      "Data type inffered for 'ProductKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: Quantity in orderrows.csv...\n",
      "Data type inffered for 'Quantity': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: UnitPrice in orderrows.csv...\n",
      "Data type inffered for 'UnitPrice': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: NetPrice in orderrows.csv...\n",
      "Data type inffered for 'NetPrice': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: UnitCost in orderrows.csv...\n",
      "Data type inffered for 'UnitCost': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.orderrows (OrderKey INT, LineNumber TINYINT, ProductKey SMALLINT, Quantity TINYINT, UnitPrice DECIMAL(18,6), NetPrice DECIMAL(18,6), UnitCost DECIMAL(18,6));...\n",
      "Begin table creation for file: orders.csv...\n",
      "Scaling factor for orders.csv: 0.002147...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\orders.csv: 8833576...\n",
      "Inferring data types for column: OrderKey in orders.csv...\n",
      "Data type inffered for 'OrderKey': INT \n",
      " \n",
      " \n",
      "Inferring data types for column: CustomerKey in orders.csv...\n",
      "Data type inffered for 'CustomerKey': MEDIUMINT \n",
      " \n",
      " \n",
      "Inferring data types for column: StoreKey in orders.csv...\n",
      "Data type inffered for 'StoreKey': MEDIUMINT \n",
      " \n",
      " \n",
      "Inferring data types for column: OrderDate in orders.csv...\n",
      "Data type inffered for 'OrderDate': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: DeliveryDate in orders.csv...\n",
      "Data type inffered for 'DeliveryDate': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: CurrencyCode in orders.csv...\n",
      "Data type inffered for 'CurrencyCode': VARCHAR(255) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.orders (OrderKey INT, CustomerKey MEDIUMINT, StoreKey MEDIUMINT, OrderDate DATE, DeliveryDate DATE, CurrencyCode VARCHAR(255));...\n",
      "Begin table creation for file: product.csv...\n",
      "Scaling factor for product.csv: 0.077606...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\product.csv: 2517...\n",
      "Inferring data types for column: ProductKey in product.csv...\n",
      "Data type inffered for 'ProductKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: ProductCode in product.csv...\n",
      "Data type inffered for 'ProductCode': MEDIUMINT \n",
      " \n",
      " \n",
      "Inferring data types for column: ProductName in product.csv...\n",
      "Data type inffered for 'ProductName': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Manufacturer in product.csv...\n",
      "Data type inffered for 'Manufacturer': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Brand in product.csv...\n",
      "Data type inffered for 'Brand': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Color in product.csv...\n",
      "Data type inffered for 'Color': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: WeightUnit in product.csv...\n",
      "Data type inffered for 'WeightUnit': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Weight in product.csv...\n",
      "Data type inffered for 'Weight': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: Cost in product.csv...\n",
      "Data type inffered for 'Cost': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: Price in product.csv...\n",
      "Data type inffered for 'Price': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: CategoryKey in product.csv...\n",
      "Data type inffered for 'CategoryKey': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: CategoryName in product.csv...\n",
      "Data type inffered for 'CategoryName': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: SubCategoryKey in product.csv...\n",
      "Data type inffered for 'SubCategoryKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: SubCategoryName in product.csv...\n",
      "Data type inffered for 'SubCategoryName': VARCHAR(255) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.product (ProductKey SMALLINT, ProductCode MEDIUMINT, ProductName VARCHAR(255), Manufacturer VARCHAR(255), Brand VARCHAR(255), Color VARCHAR(255), WeightUnit VARCHAR(255), Weight DECIMAL(18,6), Cost DECIMAL(18,6), Price DECIMAL(18,6), CategoryKey TINYINT, CategoryName VARCHAR(255), SubCategoryKey SMALLINT, SubCategoryName VARCHAR(255));...\n",
      "Begin table creation for file: sales.csv...\n",
      "Scaling factor for sales.csv: 0.001...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\sales.csv: 10000000...\n",
      "Inferring data types for column: OrderKey in sales.csv...\n",
      "Data type inffered for 'OrderKey': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: LineNumber in sales.csv...\n",
      "Data type inffered for 'LineNumber': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: OrderDate in sales.csv...\n",
      "Data type inffered for 'OrderDate': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: DeliveryDate in sales.csv...\n",
      "Data type inffered for 'DeliveryDate': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: CustomerKey in sales.csv...\n",
      "Data type inffered for 'CustomerKey': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: StoreKey in sales.csv...\n",
      "Data type inffered for 'StoreKey': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: ProductKey in sales.csv...\n",
      "Data type inffered for 'ProductKey': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: Quantity in sales.csv...\n",
      "Data type inffered for 'Quantity': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: UnitPrice in sales.csv...\n",
      "Data type inffered for 'UnitPrice': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: NetPrice in sales.csv...\n",
      "Data type inffered for 'NetPrice': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: UnitCost in sales.csv...\n",
      "Data type inffered for 'UnitCost': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: CurrencyCode in sales.csv...\n",
      "Data type inffered for 'CurrencyCode': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: ExchangeRate in sales.csv...\n",
      "Data type inffered for 'ExchangeRate': VARCHAR(255) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.sales (OrderKey VARCHAR(255), LineNumber VARCHAR(255), OrderDate VARCHAR(255), DeliveryDate VARCHAR(255), CustomerKey VARCHAR(255), StoreKey VARCHAR(255), ProductKey VARCHAR(255), Quantity VARCHAR(255), UnitPrice VARCHAR(255), NetPrice VARCHAR(255), UnitCost VARCHAR(255), CurrencyCode VARCHAR(255), ExchangeRate VARCHAR(255));...\n",
      "Begin table creation for file: store.csv...\n",
      "Scaling factor for store.csv: 0.110088...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\store.csv: 74...\n",
      "Inferring data types for column: StoreKey in store.csv...\n",
      "Data type inffered for 'StoreKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: StoreCode in store.csv...\n",
      "Data type inffered for 'StoreCode': TINYINT \n",
      " \n",
      " \n",
      "Inferring data types for column: GeoAreaKey in store.csv...\n",
      "Data type inffered for 'GeoAreaKey': SMALLINT \n",
      " \n",
      " \n",
      "Inferring data types for column: CountryCode in store.csv...\n",
      "Data type inffered for 'CountryCode': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: CountryName in store.csv...\n",
      "Data type inffered for 'CountryName': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: State in store.csv...\n",
      "Data type inffered for 'State': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: OpenDate in store.csv...\n",
      "Data type inffered for 'OpenDate': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: CloseDate in store.csv...\n",
      "Data type inffered for 'CloseDate': DATE \n",
      " \n",
      " \n",
      "Inferring data types for column: Description in store.csv...\n",
      "Data type inffered for 'Description': VARCHAR(255) \n",
      " \n",
      " \n",
      "Inferring data types for column: SquareMeters in store.csv...\n",
      "Data type inffered for 'SquareMeters': DECIMAL(18,6) \n",
      " \n",
      " \n",
      "Inferring data types for column: Status in store.csv...\n",
      "Data type inffered for 'Status': VARCHAR(255) \n",
      " \n",
      " \n",
      "Executing  CREATE TABLE IF NOT EXISTS Contoso.store (StoreKey SMALLINT, StoreCode TINYINT, GeoAreaKey SMALLINT, CountryCode VARCHAR(255), CountryName VARCHAR(255), State VARCHAR(255), OpenDate DATE, CloseDate DATE, Description VARCHAR(255), SquareMeters DECIMAL(18,6), Status VARCHAR(255));...\n",
      "Processing file: currencyexchange.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\currencyexchange.csv: 91325...\n",
      "Chunking....\n",
      "Total chunks: 92...\n",
      "Begin loading data into table `Contoso.currencyexchange`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading currencyexchange: 100%|██████████| 92/92 [00:02<00:00, 42.86chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: customer.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\customer.csv: 1679846...\n",
      "Chunking....\n",
      "Total chunks: 1680...\n",
      "Begin loading data into table `Contoso.customer`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading customer: 100%|██████████| 1680/1680 [01:50<00:00, 15.17chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: date.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\date.csv: 3653...\n",
      "Chunking....\n",
      "Total chunks: 4...\n",
      "Begin loading data into table `Contoso.date`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading date: 100%|██████████| 4/4 [00:00<00:00, 17.94chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: orderrows.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\orderrows.csv: 13042913...\n",
      "Chunking....\n",
      "Total chunks: 13043...\n",
      "Begin loading data into table `Contoso.orderrows`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading orderrows:   0%|          | 0/13043 [00:00<?, ?chunks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting chunk into table `orderrows`: Failed processing format-parameters; Python 'float64' cannot be converted to a MySQL type\n",
      "Processing file: orders.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\orders.csv: 8833576...\n",
      "Chunking....\n",
      "Total chunks: 8834...\n",
      "Begin loading data into table `Contoso.orders`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading orders: 100%|██████████| 8834/8834 [03:29<00:00, 42.15chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: product.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\product.csv: 2517...\n",
      "Chunking....\n",
      "Total chunks: 3...\n",
      "Begin loading data into table `Contoso.product`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading product: 100%|██████████| 3/3 [00:00<00:00, 17.42chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: sales.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\sales.csv: 21170416...\n",
      "Chunking....\n",
      "Total chunks: 21171...\n",
      "Begin loading data into table `Contoso.sales`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sales: 100%|██████████| 21171/21171 [12:57<00:00, 27.22chunks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: store.csv...\n",
      "Total rows in C:\\Users\\Abunch\\Downloads\\Contoso\\store.csv: 74...\n",
      "Chunking....\n",
      "Total chunks: 1...\n",
      "Begin loading data into table `Contoso.store`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading store:   0%|          | 0/1 [00:00<?, ?chunks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting chunk into table `store`: 1264 (22003): Out of range value for column 'StoreKey' at row 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import requests\n",
    "from mysql.connector.connection import MySQLConnection\n",
    "from typing import Dict,List,Tuple,Any,Optional\n",
    "import shutil\n",
    "import py7zr  # For handling .7z files\n",
    "import zipfile\n",
    "from tqdm import tqdm # progress bar for download\n",
    "import math\n",
    "\n",
    "\n",
    "# Connection Variables\n",
    "USER = 'Sudo'\n",
    "PASSWORD = 'password'\n",
    "DATABASE = 'sys' # Do not change! This is the default database for MySQL\n",
    "\n",
    "\n",
    "#Server Connection Configuration\n",
    "CONN_CONFIG: Dict[str, str]  = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": USER,\n",
    "    \"password\": PASSWORD,\n",
    "    \"database\": DATABASE\n",
    "}\n",
    "\n",
    "CONTOSO_DOWNLOAD_LINK = r\"https://github.com/sql-bi/Contoso-Data-Generator-V2-Data/releases/download/ready-to-use-data/csv-10m.7z\"\n",
    "CONTOSO_FILENAME = \"csv-10m.7z\"# DO NOT CHANGE THIS VALUE!\n",
    "\n",
    "DBSTART = 'Contoso' # Name of the database to create and where all other tables will be created\n",
    "EXTRACT_DIR = f\"{os.path.expanduser('~')}\\\\Downloads\\\\{DBSTART}\\\\\"\n",
    "\n",
    "\n",
    "def download_file(url: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from a given URL and saves it to the user's Downloads folder,\n",
    "    with a progress bar displayed in the console.\n",
    "\n",
    "    :param url: The URL of the file to download.\n",
    "    :param filename: The name of the file to save.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    user_downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    file_path = os.path.join(user_downloads_dir, filename)\n",
    "\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx or 5xx)\n",
    "\n",
    "        # Get the total file size from the headers\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "                for chunk in response.iter_content(chunk_size=1024):  # Download in 1KB chunks\n",
    "                    f.write(chunk)\n",
    "                    progress_bar.update(len(chunk))  # Update the progress bar\n",
    "\n",
    "    print(f\"File downloaded to: {file_path}\")\n",
    "\n",
    "\n",
    "def extract_archive(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Searches for a given .7z archive file in the user's Downloads folder,\n",
    "    and extracts its contents to DOWNLOADS_DIR with a progress bar and detailed error handling.\n",
    "\n",
    "    :param filename: The name of the .7z archive file to extract.\n",
    "    :return: The path where files were extracted or None if extraction failed.\n",
    "    \"\"\"\n",
    "    user_downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "    file_path = os.path.join(user_downloads_dir, filename)\n",
    "\n",
    "    # Check if the file exists in Downloads folder\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{filename}' not found in {user_downloads_dir}.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure the extraction directory exists\n",
    "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "    try:    \n",
    "        if filename.endswith(\".7z\"):\n",
    "            # Extract .7z files with a progress bar\n",
    "            try:\n",
    "                counter = 0 \n",
    "                with py7zr.SevenZipFile(file_path, mode='r', blocksize=1024*1024 ) as archive:\n",
    "                    file_list = archive.getnames()\n",
    "                    \n",
    "                    with tqdm(total=len(file_list), unit=\"Files Extracted\", desc=\"Extracting\") as  files_progress_bar:\n",
    "                \n",
    "                        for file in file_list:\n",
    "                            try:\n",
    "                                files_progress_bar.set_description(f\"Extracting {file}...\")\n",
    "                                files_progress_bar.refresh()\n",
    "                                archive.extract(targets=[os.path.join(user_downloads_dir,CONTOSO_FILENAME),file], path=EXTRACT_DIR, recursive=False)\n",
    "                                archive.reset() \n",
    "                                files_progress_bar.update(1)\n",
    "                                \n",
    "                            except Exception as file_error:\n",
    "                                print(f\"Error extracting file '{file}': {file_error}\")\n",
    "                                continue\n",
    "                            \n",
    "                            counter += 1\n",
    "                            if counter == len(file_list):\n",
    "                                archive.close()\n",
    "                                break\n",
    "                \n",
    "                print(f\"Extracted '{filename}' to '{EXTRACT_DIR}'.\")\n",
    "                return EXTRACT_DIR\n",
    "           \n",
    "            except py7zr.Bad7zFile:\n",
    "                print(f\"Error: '{filename}' is not a valid 7z file.\")\n",
    "                return None\n",
    "           \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting 7z file '{filename}': {e}\")\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: '{filename}' is not a supported archive format (7z).\")\n",
    "            return None\n",
    "\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied while accessing '{filename}' or writing to '{EXTRACT_DIR}'.\")\n",
    "        return None\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found during extraction.\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while extracting '{filename}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_database(db_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new MySQL database if it does not exist.\n",
    "\n",
    "    :param db_name: Name of the database to create.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to MySQL Server (without specifying a database)\n",
    "        conn: MySQLConnection = mysql.connector.connect(**CONN_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create database if it doesn't exist\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS `{db_name}`;\")\n",
    "        print(f\"Database `{db_name}` created or already exists.\")\n",
    "\n",
    "        # Close connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "\n",
    "def infer_mysql_dtype(series: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Infers the MySQL data type based on an analysis of a column's values from CSV data.\n",
    "    \"\"\"\n",
    "    \n",
    "    datetime_formats = {\n",
    "        \"DATETIME\": [\n",
    "            \"%Y-%m-%d %H:%M:%S\",\n",
    "            \"%d-%m-%Y %H:%M:%S\",\n",
    "            \"%m/%d/%Y %H:%M:%S\"\n",
    "        ],\n",
    "        \"DATE\": [\n",
    "            \"%Y-%m-%d\",\n",
    "            \"%d-%m-%Y\",\n",
    "            \"%m/%d/%Y\",\n",
    "            \"%d %b %Y\",\n",
    "            \"%d %B %Y\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    non_null_values = series.dropna()\n",
    "    # default if column is empty\n",
    "    if non_null_values.empty:\n",
    "        return \"VARCHAR(255)\"\n",
    "      \n",
    "    def check_datetime(value):\n",
    "        for fmt in datetime_formats[\"DATETIME\"]:\n",
    "            try:\n",
    "                pd.to_datetime(value, format=fmt)\n",
    "                return \"DATETIME\"\n",
    "          \n",
    "            except ValueError:\n",
    "                continue\n",
    "     \n",
    "        for fmt in datetime_formats[\"DATE\"]:\n",
    "            try:\n",
    "                pd.to_datetime(value, format=fmt)\n",
    "                return \"DATE\"\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    inferred_types = {\"int\": 0, \"float\": 0, \"datetime\": 0, \"date\": 0, \"str\": 0, \"bit\":0}\n",
    "    max_str_len = 0\n",
    "    is_bit_candidate = None\n",
    "\n",
    "    for x in non_null_values:\n",
    "        val = x # force native python type\n",
    "        \n",
    "        if isinstance(val, bool):\n",
    "            inferred_types[\"bit\"] += 1\n",
    "        \n",
    "        elif isinstance(val, int) and val in (0, 1):\n",
    "            inferred_types[\"bit\"] += 1\n",
    "        \n",
    "        else:\n",
    "            is_bit_candidate = False \n",
    "        \n",
    "        \n",
    "        if isinstance(val, int):\n",
    "            inferred_types[\"int\"] += 1\n",
    "        \n",
    "        elif isinstance(val, float):\n",
    "            inferred_types[\"float\"] += 1\n",
    "        \n",
    "        elif isinstance(val, str):\n",
    "            max_str_len = max(max_str_len, len(val))\n",
    "            dt_type = check_datetime(val)\n",
    "        \n",
    "            if dt_type == \"DATETIME\":\n",
    "                inferred_types[\"datetime\"] += 1\n",
    "        \n",
    "            elif dt_type == \"DATE\":\n",
    "                inferred_types[\"date\"] += 1\n",
    "        \n",
    "            else:\n",
    "                inferred_types[\"str\"] += 1\n",
    "\n",
    "    # Determine dominant type\n",
    "    \n",
    "    if is_bit_candidate and inferred_types[\"bit\"] == len(non_null_values):\n",
    "        return \"BIT\"\n",
    "    \n",
    "    if inferred_types[\"int\"] == len(non_null_values):\n",
    "        min_val = non_null_values.min()\n",
    "        max_val = non_null_values.max()\n",
    "        \n",
    "        if 0 <= min_val and max_val <= 255:\n",
    "            return \"TINYINT\"\n",
    "        \n",
    "        elif -32768 <= min_val and max_val <= 32767:\n",
    "            return \"SMALLINT\"\n",
    "        \n",
    "        elif -8388608 <= min_val and max_val <= 8388607:\n",
    "            return \"MEDIUMINT\"\n",
    "        \n",
    "        elif -2147483648 <= min_val and max_val <= 2147483647:\n",
    "            return \"INT\"\n",
    "        \n",
    "        else:\n",
    "            return \"BIGINT\"\n",
    "\n",
    "    elif inferred_types[\"float\"] + inferred_types[\"int\"] == len(non_null_values):\n",
    "        return \"DECIMAL(18,6)\"\n",
    "\n",
    "    elif inferred_types[\"datetime\"] > 0 and inferred_types[\"datetime\"] + inferred_types[\"str\"] == len(non_null_values):\n",
    "        return \"DATETIME\"\n",
    "    \n",
    "    elif inferred_types[\"date\"] > 0 and inferred_types[\"date\"] + inferred_types[\"str\"] == len(non_null_values):\n",
    "        return \"DATE\"\n",
    "\n",
    "    elif inferred_types[\"str\"] > 0:\n",
    "    \n",
    "        if max_str_len <= 255:\n",
    "            return f\"VARCHAR(255)\"\n",
    "    \n",
    "        elif max_str_len <= 65535:\n",
    "            return \"TEXT\"\n",
    "    \n",
    "        else:\n",
    "            return \"LONGTEXT\"\n",
    "\n",
    "    return \"VARCHAR(255)\"\n",
    "\n",
    "\n",
    "def count_rows_in_csv(filepath: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    :param filepath: Path to the CSV file.\n",
    "    :return: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            total_rows = sum(1 for _ in file) - 1  # Subtract 1 for the header row\n",
    "        return total_rows\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        return 0\n",
    " \n",
    "\n",
    "def compute_scale(x: int, min_x: int = 0, max_x: int = 10000000, min_scale: float = 0.001, max_scale: float = 0.15) -> float:\n",
    "    if not (min_x <= x <= max_x):\n",
    "        raise ValueError(\"Input must be between 0 and 10,000,000\")\n",
    "\n",
    "    # Normalize using log scale to emphasize early values\n",
    "    log_x = math.log10(x + 1)  # avoid log(0)\n",
    "    log_max = math.log10(max_x + 1)\n",
    "    scale = max_scale - (log_x / log_max) * (max_scale - min_scale)\n",
    "    \n",
    "    return round(scale, 6)\n",
    "\n",
    "\n",
    "def create_tables_from_csv(targetdirectory: str = EXTRACT_DIR ) -> None:\n",
    "    \"\"\"\n",
    "    Scans CSV files in a given directory, infers table schema, \n",
    "    and creates MySQL tables dynamically based on CSV headers and data types.\n",
    "    then loads the data into the tables.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(targetdirectory, \"*.csv\"))\n",
    "    file_list = [(os.path.basename(file), file) for file in csv_files]\n",
    "    chunk_size = 1000  # Number of rows per chunk\n",
    "    conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(f\"Found {len(file_list)} CSV files in {targetdirectory}.\")\n",
    "\n",
    "    #######################\n",
    "    # Table Creation Loop #\n",
    "    #######################\n",
    "    for filename, filepath in file_list:\n",
    "\n",
    "        print(f\"Begin table creation for file: {filename}...\")\n",
    "        \n",
    "        table_name = os.path.splitext(filename)[0]  # Remove .csv extension\n",
    "\n",
    "        df = pd.read_csv(filepath,nrows=1)\n",
    "        columns = df.columns.tolist()\n",
    "        total_row_count = count_rows_in_csv(filepath)\n",
    "        total_row_count = min(total_row_count, 10000000)  # Limit to compute scale max_x default.\n",
    "        scaling_factor = compute_scale(x=total_row_count)\n",
    "        inferred_types = []\n",
    "        \n",
    "        print(f\"Scaling factor for {filename}: {scaling_factor}...\")\n",
    "        print(f\"Total rows in {filepath}: {total_row_count}...\")\n",
    "        \n",
    "        for col in columns:\n",
    "            print(f\"Inferring data types for column: {col} in {filename}...\")\n",
    "            column_types = [] \n",
    "\n",
    "            with pd.read_csv(filepath, usecols=[col], chunksize=1000) as data:\n",
    "                \n",
    "                for chunk in data:\n",
    "                    chunk = chunk.sample(frac=scaling_factor)\n",
    "                    column_types.extend([infer_mysql_dtype(chunk[col])])\n",
    "            \n",
    "            # prefer VARCHAR over INT\n",
    "            if \"VARCHAR(255)\" in column_types:\n",
    "                most_frequent_type = \"VARCHAR(255)\"\n",
    "            \n",
    "            else:\n",
    "                most_frequent_type = max(set(column_types), key=column_types.count)\n",
    "            \n",
    "            inferred_types.append(most_frequent_type)\n",
    "            print(f\"Data type inffered for '{col}': {most_frequent_type} \\n \\n \")\n",
    "            \n",
    "        \n",
    "        columns_sql = \", \".join(f\"{col} {dtype}\" for col, dtype in zip(columns, inferred_types))\n",
    "        create_table_sql = f\" CREATE TABLE IF NOT EXISTS {DBSTART}.{table_name} ({columns_sql});\"\n",
    "        \n",
    "        print(f\"Executing {create_table_sql}...\")\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "        #Confirm the table was created in the SQL server\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT EXISTS(\n",
    "                SELECT * FROM information_schema.tables \n",
    "                WHERE\n",
    "                    table_type = 'BASE TABLE'\n",
    "                    AND table_name = '{table_name}');\n",
    "            \"\"\"\n",
    "        )\n",
    "        table_check = cursor.fetchall()        \n",
    "        \n",
    "        try:\n",
    "            if len(table_check) == 1 & table_check[0][0] == table_name:\n",
    "                print(f\"Table `{table_name}` created successfully. \\n \\n \")\n",
    "        except Exception as err:\n",
    "            print(f\"Table creation unsuccessfull, Error: {err}\")\n",
    "            \n",
    "        \n",
    "    #####################\n",
    "    # Table Insert Loop #\n",
    "    #####################\n",
    "    for filename, filepath in file_list:            \n",
    "    \n",
    "        table_name = os.path.splitext(filename)[0] \n",
    "        \n",
    "        print(f\"Processing file: {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath,'r') as file:\n",
    "                total_rows = sum(1 for _ in file) - 1 # Subtract 1 for the header row\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File '{filepath}' not found.\")\n",
    "        \n",
    "        print(f\"Total rows in {filepath}: {total_rows}...\")\n",
    "        print(f\"Chunking....\")     \n",
    "        \n",
    "        total_chunks = math.ceil(total_rows / chunk_size)  # Number of chunks \n",
    "        \n",
    "        print(f\"Total chunks: {total_chunks}...\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Begin loading data into table `{DBSTART}.{table_name}`...\")\n",
    "            #load the CSV to dataframe and load in chunks\n",
    "            with pd.read_csv(filepath, chunksize=chunk_size) as data:\n",
    "                    \n",
    "                with tqdm(total=total_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "                    try:\n",
    "                        for chunk in data:\n",
    "                            columns = chunk.columns.tolist()\n",
    "                            records = [tuple(row) for row in chunk.to_numpy()]\n",
    "                            insert_stmt = f\"\"\"\n",
    "                                INSERT INTO {DBSTART}.{table_name} ({ \", \".join(f\"{col}\" for col in columns)}) \n",
    "                                VALUES ({','.join(['%s'] * len(chunk.columns))});\n",
    "                                \"\"\"\n",
    "                            # print(insert_stmt)\n",
    "                            cursor.executemany(insert_stmt, records)\n",
    "                            conn.commit()\n",
    "                            progress_bar.update(1)\n",
    "                            \n",
    "                    except mysql.connector.Error as err:\n",
    "                        print(f\"Error inserting chunk into table `{table_name}`: {err}\")\n",
    "\n",
    "                    except Exception as err:\n",
    "                        print(f\"Error loading chunk : {err}\")\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"Error loading data into table `{table_name}`: {err}\")     \n",
    "               \n",
    "    # Close DB connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_table(databse_name: str, table_name: str, columns: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Creates a new MySQL table with the specified columns.\n",
    "\n",
    "    :param table_name: The name of the table to create.\n",
    "    :param columns: A dictionary where keys are column names and values are MySQL data types.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        columns_sql = \", \".join(f\"`{col}` {dtype}\" for col, dtype in columns.items())\n",
    "\n",
    "        create_table_sql = f\"CREATE TABLE IF NOT EXISTS `{databse_name}.{table_name}` ({columns_sql});\"\n",
    "\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "         #Confirm the table was created in the SQL server\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT EXISTS(\n",
    "                SELECT * FROM information_schema.tables \n",
    "                WHERE\n",
    "                        table_type = 'BASE TABLE'\n",
    "                        AND table_name = '{table_name}');\n",
    "            \"\"\"\n",
    "        )\n",
    "        table_check = cursor.fetchall()        \n",
    "        try:\n",
    "            if len(table_check) == 1 & table_check[0][0] == table_name:\n",
    "                print(f\"Table `{table_name}` created successfully.\")\n",
    "        except Exception as err:\n",
    "            print(f\"Table not created successfully, Error: {err}\")\n",
    "            \n",
    "\n",
    "        # Close DB connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "\n",
    "    \n",
    "# def load_to_table(table_name : str) -> None:\n",
    "        \n",
    "#     try:\n",
    "#         #load the CSV to dataframe and index in blocks of 100 rows\n",
    "#         data_df = pd.read_csv(filepath, chunksize=1000)\n",
    "#         # find the number of chunks\n",
    "#         num_chunks = sum(1 for _ in data_df)\n",
    "#         counter = 0\n",
    "        \n",
    "#         with tqdm(total=num_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "\n",
    "#             for chunk in data_df:\n",
    "#                 try:\n",
    "#                     chunk.to_sql(f\"{DBSTART}.{table_name}\", conn, if_exists='append', index=False)\n",
    "#                     print(f\"Chunk of data loaded into table `{table_name}` successfully.\")\n",
    "#                     counter += 1\n",
    "                \n",
    "#                 except Exception as err:\n",
    "#                     print(f\"Error loading chunk # {counter} into table `{table_name}`: {err}\")\n",
    "#                     break\n",
    "                    \n",
    "        \n",
    "#             # Load data into the table\n",
    "#             df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "#             print(f\"Data loaded into table `{table_name}` successfully.\")\n",
    "        \n",
    "    \n",
    "#     except Exception as err:\n",
    "#         print(f\"Error loading data into table `{table_name}`: {err}\")\n",
    "\n",
    "# download_file(CONTOSO_DOWNLOAD_LINK, CONTOSO_FILENAME) # works\n",
    "# extract_archive(CONTOSO_FILENAME) # works\n",
    "# create_database(DBSTART) # works\n",
    "create_tables_from_csv(EXTRACT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: date.csv...\n",
      "Total rows in C:\\\\Users\\\\Abunch\\\\Downloads\\\\Contoso\\\\date.csv: 3653...\n",
      "Chunking....\n",
      "Total chunks: 37...\n",
      "Begin loading data into table `Contoso.date`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading date: 100%|██████████| 37/37 [00:00<00:00, 66.92chunks/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "csv_files = [r\"C:\\\\Users\\\\Abunch\\\\Downloads\\\\Contoso\\\\orderrows.csv\"]        \n",
    "file_list = [(os.path.basename(file), file) for file in csv_files]\n",
    "conn = mysql.connector.connect(**CONN_CONFIG)\n",
    "cursor = conn.cursor()\n",
    "# Table Insert Loop\n",
    "for filename, filepath in file_list:            \n",
    "    \n",
    "    table_name = os.path.splitext(filename)[0] \n",
    "    \n",
    "    print(f\"Processing file: {filename}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath,'r') as file:\n",
    "            total_rows = sum(1 for _ in file) - 1 # Subtract 1 for the header row\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        total_rows = 0\n",
    "    \n",
    "    print(f\"Total rows in {filepath}: {total_rows}...\")\n",
    "    print(f\"Chunking....\")     \n",
    "    \n",
    "    chunk_size = 100  # Number of rows per chunk\n",
    "    total_chunks = math.ceil(total_rows / chunk_size)  # Number of chunks \n",
    "    counter = 1\n",
    "    \n",
    "    print(f\"Total chunks: {total_chunks}...\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Begin loading data into table `{DBSTART}.{table_name}`...\")\n",
    "        #load the CSV to dataframe and load in chunks\n",
    "        with pd.read_csv(filepath, chunksize=chunk_size) as data:\n",
    "                \n",
    "            with tqdm(total=total_chunks, unit=\"chunks\", desc=f\"Loading {table_name}\") as progress_bar:\n",
    "                try:\n",
    "                    for chunk in data:\n",
    "                        columns = chunk.columns.tolist()\n",
    "                        records = [tuple(row) for row in chunk.to_numpy()]\n",
    "                        insert_stmt = f\"\"\"\n",
    "                            INSERT INTO {DBSTART}.{table_name} ({ \", \".join(f\"{col}\" for col in columns)}) \n",
    "                            VALUES ({','.join(['%s'] * len(chunk.columns))})\n",
    "                            \"\"\"\n",
    "                        # print(insert_stmt)\n",
    "                        cursor.executemany(insert_stmt, records)\n",
    "                        conn.commit()\n",
    "                        progress_bar.update(1)\n",
    "                        counter += 1\n",
    "                        \n",
    "                except mysql.connector.Error as err:\n",
    "                    print(f\"Error inserting chunk into table `{table_name}`: {err}\")\n",
    "                    break\n",
    "\n",
    "                except Exception as err:\n",
    "                    print(f\"Error loading chunk : {err}\")\n",
    "                    break\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error loading data into table `{table_name}`: {err}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
